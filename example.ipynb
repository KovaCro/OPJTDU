{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_ENGLISH_TRAIN = 'en_partut-ud-train.conllu'\n",
    "UD_ENGLISH_DEV = 'en_partut-ud-dev.conllu'\n",
    "UD_ENGLISH_TEST = 'en_partut-ud-test.conllu'\n",
    "embs_path = 'wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "def read_conllu(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    tagged_sentences=[]\n",
    "    for sentence in data:\n",
    "        tagged_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token.upos:\n",
    "                tagged_sentence.append((token.form.lower(), token.upos))\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format('datasets/' + embs_path, binary=False)\n",
    "\n",
    "train_sentences = read_conllu('datasets/' + UD_ENGLISH_TRAIN)\n",
    "val_sentences = read_conllu('datasets/' + UD_ENGLISH_DEV)\n",
    "test_sentences = read_conllu('datasets/' + UD_ENGLISH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)\n",
    "oov =  np.random.uniform(-0.25, 0.25, dim) # out of vocabulary vector\n",
    "\n",
    "def features_embs(sentence, index, window):\n",
    "    vec = np.array([])\n",
    "    for i in range(index-window,index+window+1):\n",
    "        if i < 0:\n",
    "            vec = np.append(vec, pad)\n",
    "            continue\n",
    "        if i > len(sentence) - 1:\n",
    "            vec = np.append(vec, pad)\n",
    "            continue\n",
    "        try:\n",
    "            vec = np.append(vec, embeddings[sentence[i]])\n",
    "        except:\n",
    "            vec = np.append(vec, oov)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import untag\n",
    "\n",
    "def transform_to_dataset(tagged_sentences, window):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features_embs(untag(tagged), index, window))\n",
    "            y.append(tagged[index][1])\n",
    "    return X, y\n",
    "\n",
    "def vectorize(train, val, test, window):\n",
    "    X_train, y_train = transform_to_dataset(train, window)\n",
    "    X_val, y_val = transform_to_dataset(val, window)   \n",
    "    X_test, y_test = transform_to_dataset(test, window)\n",
    "    \n",
    "    return (\n",
    "        np.asarray(X_train), \n",
    "        np.asarray(y_train), \n",
    "        np.asarray(X_val), \n",
    "        np.asarray(y_val), \n",
    "        np.asarray(X_test), \n",
    "        np.asarray(y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43503, 900)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = vectorize(train_sentences, val_sentences, test_sentences, 1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.8832    0.7803    0.8286       223\n",
      "         ADP     0.9452    0.8832    0.9131       488\n",
      "         ADV     0.9057    0.7500    0.8205       128\n",
      "         AUX     0.9286    1.0000    0.9630       234\n",
      "       CCONJ     1.0000    0.9896    0.9948        96\n",
      "         DET     0.9661    0.9749    0.9705       439\n",
      "        INTJ     1.0000    1.0000    1.0000         2\n",
      "          NN     0.0000       nan    0.0000         0\n",
      "        NOUN     0.9549    0.8143    0.8790       754\n",
      "         NUM     0.9649    0.9016    0.9322        61\n",
      "        PART     0.5323    1.0000    0.6947        66\n",
      "        PRON     0.9333    0.7706    0.8442       109\n",
      "       PROPN     0.8676    0.6556    0.7468        90\n",
      "       PUNCT     1.0000    1.0000    1.0000       339\n",
      "       SCONJ     0.6863    0.6863    0.6863        51\n",
      "        VERB     0.9316    0.6687    0.7786       326\n",
      "           X     1.0000    1.0000    1.0000         2\n",
      "\n",
      "    accuracy                         0.8603      3408\n",
      "   macro avg     0.8529    0.8672    0.8266      3408\n",
      "weighted avg     0.9352    0.8603    0.8917      3408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "from utils import tag_list, apply_tagger, tag_sequence\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "unigram_tagger = nltk.UnigramTagger(train_sentences+val_sentences, backoff=default_tagger)\n",
    "\n",
    "y_test = [item for sublist in tag_sequence(test_sentences) for item in sublist]\n",
    "y_pred = tag_list(apply_tagger(unigram_tagger, test_sentences))\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4, zero_division=np.nan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "#from torch.nn.functional import one_hot\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = vectorize(train_sentences, val_sentences, test_sentences, 1)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "#y_train = one_hot(torch.tensor(y_train).long())\n",
    "#y_test = one_hot(torch.tensor(y_test).long())\n",
    "#y_val = one_hot(torch.tensor(y_val).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 144.77it/s, accuracy=0.937, loss=0.309, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.377565783533183, Val Accuracy: 0.8844836218790575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 133.35it/s, accuracy=0.91, loss=0.319, lr=0.0008]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.3028968261046843, Val Accuracy: 0.9104069173336029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 168.57it/s, accuracy=0.91, loss=0.29, lr=0.00064]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.2383726266297427, Val Accuracy: 0.9262199212204326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 160.13it/s, accuracy=0.973, loss=0.12, lr=0.000512]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Val Loss: 0.2345898984508081, Val Accuracy: 0.9236296794631265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 113.90it/s, accuracy=0.955, loss=0.101, lr=0.00041] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Val Loss: 0.2193894230506637, Val Accuracy: 0.9281625991517847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 140.53it/s, accuracy=0.973, loss=0.0916, lr=0.000328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Val Loss: 0.20534061843698675, Val Accuracy: 0.936685326424512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 138.31it/s, accuracy=0.964, loss=0.141, lr=0.000262] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Val Loss: 0.192309402606704, Val Accuracy: 0.9435369318181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 144.85it/s, accuracy=0.964, loss=0.105, lr=0.00021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Val Loss: 0.1989999105307189, Val Accuracy: 0.9413018036972393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 137.61it/s, accuracy=0.991, loss=0.0454, lr=0.000168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Val Loss: 0.18485583348030393, Val Accuracy: 0.9424715909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:02<00:00, 133.22it/s, accuracy=0.964, loss=0.0997, lr=0.000134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Val Loss: 0.194587189365517, Val Accuracy: 0.9408631351861086\n"
     ]
    }
   ],
   "source": [
    "import kan\n",
    "from importlib import reload\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "reload(kan)\n",
    "\n",
    "model = kan.KAN([X_train.shape[1], 64, y_train.max()+1])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader([(torch.from_numpy(x).to(torch.float), y) for x, y in zip(X_train, y_train)], batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader([(torch.from_numpy(x).to(torch.float), y) for x, y in zip(X_val, y_val)], batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader([(torch.from_numpy(x).to(torch.float), y) for x, y in zip(X_test, y_test)], batch_size=128, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for i, (x, y) in enumerate(pbar):\n",
    "            x = x.view(-1, X_train.shape[1]).to(device)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = (output.argmax(dim=1) == y.to(device)).float().mean()\n",
    "            pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.view(-1, X_train.shape[1]).to(device)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            output = model(x)\n",
    "            val_loss += criterion(output, y.to(device)).item()\n",
    "            val_accuracy += ((output.argmax(dim=1) == y.to(device)).float().mean().item())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy /= len(val_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.194587189365517, Test Accuracy: 0.9562500008830318\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader([(torch.from_numpy(x).to(torch.float), y) for x, y in zip(X_test, y_test)], batch_size=128, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, X_train.shape[1]).to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        output = model(images)\n",
    "        test_loss += criterion(output, labels.to(device)).item()\n",
    "        test_acc += (\n",
    "            (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
    "        )\n",
    "test_loss /= len(test_loader)\n",
    "test_acc /= len(test_loader)\n",
    "\n",
    "print(\n",
    "    f\"Test Loss: {val_loss}, Test Accuracy: {test_acc}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
